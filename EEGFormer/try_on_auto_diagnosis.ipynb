{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from copy import copy\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy.random import RandomState\n",
    "import resampy\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import elu\n",
    "from torch import nn\n",
    "#from torch.nn import Identity\n",
    "\n",
    "from braindecode.datautil.signal_target import SignalAndTarget\n",
    "from braindecode.torch_ext.util import np_to_var\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from braindecode.torch_ext.modules import Expression\n",
    "from braindecode.experiments.experiment import Experiment\n",
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "from braindecode.experiments.monitors import (RuntimeMonitor, LossMonitor,\n",
    "                                              MisclassMonitor)\n",
    "\n",
    "from braindecode.experiments.stopcriteria import MaxEpochs\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from braindecode.models.deep4 import Deep4Net\n",
    "from braindecode.models.util import to_dense_prediction_model\n",
    "from braindecode.datautil.iterators import get_balanced_batches\n",
    "from braindecode.torch_ext.constraints import MaxNormDefaultConstraint\n",
    "from braindecode.torch_ext.util import var_to_np\n",
    "from braindecode.torch_ext.functions import identity\n",
    "\n",
    "from dataset import DiagnosisSet\n",
    "from monitors import compute_preds_per_trial, CroppedDiagnosisMonitor\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel('DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There should always be a 'train' and 'eval' folder directly\n",
    "# below these given folders\n",
    "# Folders should contain all normal and abnormal data files without duplications\n",
    "data_folders = [\n",
    "    '/home/arooba/ssd/hira/nmt dataset/nmt_scalp_eeg_dataset/normal',\n",
    "    '/home/arooba/ssd/hira/nmt dataset/nmt_scalp_eeg_dataset/abnormal']\n",
    "n_recordings = 14  # set to an integer, if you want to restrict the set size\n",
    "sensor_types = [\"EEG\"]\n",
    "n_chans = 21\n",
    "max_recording_mins = 16 # exclude larger recordings from training set\n",
    "sec_to_cut = 60  # cut away at start of each recording\n",
    "duration_recording_mins = 10  # how many minutes to use per recording\n",
    "test_recording_mins = 10\n",
    "max_abs_val = 800  # for clipping\n",
    "sampling_freq = 100\n",
    "divisor = 10  # divide signal by this\n",
    "test_on_eval = True  # test on evaluation set or on training set\n",
    "# in case of test on eval, n_folds and i_testfold determine\n",
    "# validation fold in training set for training until first stop\n",
    "n_folds = 10\n",
    "i_test_fold = 9\n",
    "shuffle = True\n",
    "model_name = 'transformer'#shallow/deep for DNN (deep terminal local 1)\n",
    "n_start_chans = 25\n",
    "n_chan_factor = 2  # relevant for deep model only\n",
    "input_time_length = 200 #=========\n",
    "final_conv_length = 1\n",
    "model_constraint = 'defaultnorm'\n",
    "init_lr = 1e-3\n",
    "batch_size = 64\n",
    "max_epochs = 35 # until first stop, the continue train on train+valid\n",
    "cuda = False\n",
    "num_cols = 60032  #===========\n",
    "\n",
    "B = 128  # batch size \n",
    "S = n_chans  # channels\n",
    "C = 120  # convolutional dimension depth\n",
    "L = num_cols//B  # segment length  #========== this turns out to be 469\n",
    "M = L // 5  # reduced temporal dimension\n",
    "num_heads = 7 # divides the temporal dim L perfectly\n",
    "path_to_label_ref_csv = r\"C:\\Users\\DELL\\Downloads\\tukl\\Implementations\\eegformer\\dataset_s\\new.csv\"\n",
    "root_dir = r\"C:\\Users\\DELL\\Downloads\\tukl\\Implementations\\eegformer\\dataset_s\\*\\*\\*.edf\"\n",
    "dropout = 0.01\n",
    "eval_iters = 1\n",
    "eval_interval = 3\n",
    "log_file_path = \"loss_log.txt\"\n",
    "max_iters = 469  # 469 * batch_size = 60032  # for one example\n",
    "learning_rate = 0.01\n",
    "dropout = 0.01\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set(X, y, inds):\n",
    "    \"\"\"\n",
    "    Parameters:\t\n",
    "    X (3darray or list of 2darrays) – The input signal per trial.\n",
    "    y (1darray or list) – Labels for each trial.\n",
    "    \"\"\"\n",
    "    new_X = []\n",
    "    for i in inds:\n",
    "        new_X.append(X[i])\n",
    "    new_y = y[inds]\n",
    "    return (torch.tensor(new_X).double().to(device), torch.tensor(new_y).double().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValidTestSplitter(object):\n",
    "    def __init__(self, n_folds, i_test_fold, shuffle):\n",
    "        self.n_folds = n_folds\n",
    "        self.i_test_fold = i_test_fold\n",
    "        self.rng = RandomState(39483948)\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def split(self, X, y):\n",
    "        '''\n",
    "        X: 3d array\n",
    "        y: numpy list\n",
    "        '''\n",
    "        if len(X) < self.n_folds:\n",
    "            raise ValueError(\"Less Trials: {:d} than folds: {:d}\".format(\n",
    "                len(X), self.n_folds\n",
    "            ))\n",
    "        folds = get_balanced_batches(len(X), self.rng, self.shuffle,\n",
    "                                     n_batches=self.n_folds)\n",
    "        test_inds = folds[self.i_test_fold]\n",
    "        valid_inds = folds[self.i_test_fold - 1]\n",
    "        all_inds = list(range(len(X)))\n",
    "        # print(all_inds)\n",
    "        # print(test_inds)\n",
    "        # print(valid_inds)\n",
    "        train_inds = np.setdiff1d(all_inds, np.union1d(test_inds, valid_inds))\n",
    "\n",
    "        train_set = create_set(X, y, train_inds)\n",
    "        valid_set = create_set(X, y, valid_inds)\n",
    "        test_set = create_set(X, y, test_inds)\n",
    "\n",
    "        return train_set, valid_set, test_set\n",
    "    \n",
    "# splitter = TrainValidTestSplitter(3, 2, True)\n",
    "# splitter.split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValidSplitter(object):\n",
    "    def __init__(self, n_folds, i_valid_fold, shuffle):\n",
    "        self.n_folds = n_folds\n",
    "        self.i_valid_fold = i_valid_fold\n",
    "        self.rng = RandomState(39483948)\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def split(self, X, y):\n",
    "        '''\n",
    "        X: 3d array\n",
    "        y: numpy list\n",
    "        '''\n",
    "        if len(X) < self.n_folds:\n",
    "            raise ValueError(\"Less Trials: {:d} than folds: {:d}\".format(\n",
    "                len(X), self.n_folds\n",
    "            ))\n",
    "        folds = get_balanced_batches(len(X), self.rng, self.shuffle,\n",
    "                                     n_batches=self.n_folds)\n",
    "        valid_inds = folds[self.i_valid_fold]\n",
    "        all_inds = list(range(len(X)))\n",
    "        train_inds = np.setdiff1d(all_inds, valid_inds)\n",
    "        assert np.intersect1d(train_inds, valid_inds).size == 0\n",
    "        assert np.array_equal(np.sort(np.union1d(train_inds, valid_inds)),\n",
    "            all_inds)\n",
    "\n",
    "        train_set = create_set(X, y, train_inds)\n",
    "        valid_set = create_set(X, y, valid_inds)\n",
    "        return train_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(tensor, dim, length):\n",
    "    tensor_shape = list(tensor.shape)\n",
    "    current_length = tensor_shape[dim]\n",
    "\n",
    "    if current_length >= length:\n",
    "        return tensor\n",
    "\n",
    "    padding_shape = tensor_shape.copy()\n",
    "    padding_shape[dim] = length - current_length\n",
    "\n",
    "    padding = torch.zeros(padding_shape, dtype=tensor.dtype, device=device)\n",
    "    padded_tensor = torch.cat((tensor, padding), dim=dim)\n",
    "\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing CUDA enabled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B = 3  # batch size\n",
    "# S = 21\n",
    "# C = 120\n",
    "# L = 150\n",
    "# M = L//5  # reduced temporal dimension\n",
    "# num_heads = 5\n",
    "# device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = CNN1D(S, C, L).to(device)\n",
    "# yoo = c(torch.rand(B, S, L).double().to(device))\n",
    "# print(yoo.shape)\n",
    "# print(yoo.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = pad_tensor(yoo, 3, L)\n",
    "# print(y.shape)\n",
    "# print(y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rh = RegionalHead(head_size=L//5, C=C, L=L).to(device)\n",
    "# rh_output = rh(torch.rand(B, S, C, L).double().to(device))  # unsqueeze adds an extra batch dimension\n",
    "# print(rh_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mrh = RegionalMultiHeadAttention(5, head_size=L//5, C=C, L=L).to('cuda')\n",
    "# mrh_output = mrh(torch.rand(B, S, C, L).double().to('cuda'))\n",
    "# mrh_output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffr = FeedFowardRegional(L).to('cuda')\n",
    "# ffr_output = ffr(torch.rand(B, S, C, L).double().to('cuda'))\n",
    "# print(ffr_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# br = BlockRegional(L, 5).to('cuda')\n",
    "# br_output = br(torch.rand(B, S, C, L).double().to('cuda'))\n",
    "# print(br_output.shape)\n",
    "# print(br_output.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def RegionalToSynchronousShapeShifter(tensor):\n",
    "#     b, s, c, l = tensor.shape\n",
    "#     return tensor.view(b, c, s, l)\n",
    "\n",
    "# sync_input = RegionalToSynchronousShapeShifter(br_output)\n",
    "# print(sync_input.shape)\n",
    "# print(sync_input.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh = SynchronousHead(head_size=L//5, S=S, L=L).to('cuda')\n",
    "# sh_output = sh(sync_input) # unsqueeze adds an extra batch dimension\n",
    "# print(sh_output.shape)\n",
    "# print(sh_output.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mhs = SynchronousMultiHeadAttention(5, head_size=L//5, S=S, L=L).to('cuda')\n",
    "# mhs_output = mhs(sync_input)\n",
    "# print(mhs_output.shape)\n",
    "# print(mhs_output.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffs = FeedFowardSync(L).to(device)\n",
    "# ffs_output = ffs(mhs_output)\n",
    "# print(ffs_output.shape)\n",
    "# print(ffs_output.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs = BlockSync(L, 5).to('cuda')\n",
    "# bs_output = bs(sync_input)\n",
    "# print(bs_output.shape)\n",
    "# print(bs_output.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal = TemporalTransformer(S, C, L, M=L//5).to(device)  # M is 15 here\n",
    "# print(f'Input: {bs_output.shape}')\n",
    "# temporal_without_attention_output = temporal(bs_output)\n",
    "# print(f'Output: {temporal_without_attention_output.shape}')\n",
    "# print(f'Output device: {temporal_without_attention_output.device}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Till now the shape of tensor is: B, N, D\n",
    "# # We don't want d to decrease, so we need to specify num_of_heads, and head_size carefully\n",
    "# # The product_of_2_least_common_factors function allows us to achieve this.\n",
    "# # It gives us a small number which can be used as the parameter: number of heads, as it's ouput divides d with no remainder\n",
    "# no_of_heads_for_temoral_block = product_of_2_least_common_factors(S*C)\n",
    "# print(no_of_heads_for_temoral_block)\n",
    "# bt = TemporalBlock(S*C, no_of_heads_for_temoral_block).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Input shape: {temporal_without_attention_output.shape}')\n",
    "# eeg_encoder_output = bt(temporal_without_attention_output)\n",
    "# print(f'Output shape: {eeg_encoder_output.shape}')\n",
    "# print(f'Output shape: {eeg_encoder_output.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder = Decoder(B, M, S, C).to(device)\n",
    "# prediction = decoder(eeg_encoder_output)\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize your EEGFormer model\n",
    "# device='cuda'\n",
    "# model = EEGFormer(B, S, C, L, M)\n",
    "# model.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp= torch.rand(B, S,L).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1DCNN BLOCK\n",
    "# ============================================\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, S=21, L=150, C=120):\n",
    "        super().__init__()\n",
    "        self.S = S  # no. of channels\n",
    "        self.L = L  # no. of sampled points\n",
    "        self.C = C  # depth of convolutional dimension\n",
    "        self.conv_layer_1 = nn.Conv1d(1, C, kernel_size=4)\n",
    "        self.conv_layer_2 = nn.Conv1d(C, C, kernel_size=4)\n",
    "        self.conv_layer_3 = nn.Conv1d(C, C, kernel_size=4)\n",
    "        self.conv_layer_4 = nn.Conv1d(C, C, kernel_size=4)\n",
    "        self.conv_layer_1.double()  # Update the data type of the convolutional layer weights to torch.float64\n",
    "        self.conv_layer_2.double()\n",
    "        self.conv_layer_3.double()\n",
    "        self.conv_layer_4.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(self.S):\n",
    "            input_row = x[:, i:i+1, :]  # (batch_size=1, channels=1, length=num_columns)\n",
    "            output_tensor = self.conv_layer_1(input_row)\n",
    "            output_tensor = self.conv_layer_2(output_tensor)\n",
    "            output_tensor = self.conv_layer_3(output_tensor)\n",
    "            output_tensor = self.conv_layer_4(output_tensor)\n",
    "            outputs.append(output_tensor.unsqueeze(1))\n",
    "\n",
    "        output_tensor = torch.cat(outputs, dim=1)\n",
    "        return output_tensor\n",
    "    \n",
    "# ============================================\n",
    "# REGIONAL ENCODER\n",
    "# ============================================\n",
    "class RegionalHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, C, L):\n",
    "        # Parameters are head_size(d), no. of tokens(C), and input embedding size(L)\n",
    "        super().__init__()\n",
    "        self.block_size = C\n",
    "        self.n_embed = L\n",
    "        self.head_size = head_size \n",
    "        self.key = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
    "        self.query = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
    "        self.value = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(self.block_size, self.block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S_, C_, L_ = x.shape\n",
    "        x = x.view(S_, B, C_, L_)  # (B, T, C, L)\n",
    "        matrices = []\n",
    "\n",
    "        for spatial_mat in x:\n",
    "            inp = spatial_mat\n",
    "            # Below this, T is not the original T, but the head size\n",
    "            k = self.key(inp)   # (B, C, T)\n",
    "            q = self.query(inp) # (B, C, T)\n",
    "            # compute attention scores (\"affinities\")\n",
    "            wei = q @ k.transpose(-2,-1) * self.head_size**-0.5 # (B, C, T) @ (B, T, C) -> (B, C, C)\n",
    "            wei = F.softmax(wei, dim=-1) # (B, C, C)\n",
    "            wei = self.dropout(wei)\n",
    "            # perform the weighted aggregation of the values\n",
    "            v = self.value(inp) # (B, C, T)\n",
    "            out = wei @ v # (B, C, C) @ (B, C, T) -> (B, C, T)\n",
    "            matrices.append(out.tolist())\n",
    "\n",
    "        matrices = torch.tensor(matrices)\n",
    "        out = matrices.view(B, S_, C_, self.head_size)\n",
    "        return out.double()\n",
    "    \n",
    "class RegionalMultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, C, L):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([RegionalHead(head_size, C, L) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(L, L).double()\n",
    "        self.dropout = nn.Dropout(0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1).to(device)\n",
    "        # out = self.dropout(self.proj(out)) # Instead of this line, we proceed as below:\n",
    "\n",
    "        # Implementing projection layer after the multihead attention module\n",
    "        b, s, c, l = out.shape\n",
    "        out = out.view(s, b, c, l)\n",
    "\n",
    "        matrices = []\n",
    "        for inp in out:\n",
    "            matrix = self.dropout(self.proj(inp))\n",
    "            matrices.append(matrix.tolist())\n",
    "\n",
    "        matrices = torch.tensor(matrices)\n",
    "        matrices = matrices.view(b, s, c, l).to('cuda')\n",
    "\n",
    "        return matrices.double()\n",
    "    \n",
    "class FeedFowardRegional(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, L): \n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(L, 4*L), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*L, L),\n",
    "            nn.Dropout(0.001),\n",
    "        ).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, s, c, l = x.shape\n",
    "        x = x.view(s, b, c, l)\n",
    "\n",
    "        matrices = []\n",
    "        for inp in x:\n",
    "            matrix = self.net(inp)\n",
    "            matrices.append(matrix.tolist())\n",
    "\n",
    "        matrices = torch.tensor(matrices).to(device)\n",
    "        # s_, b_, c_, l_ = matrices.shape\n",
    "        matrices = matrices.view(b, s, c, l)\n",
    "\n",
    "\n",
    "        return matrices.double()\n",
    "    \n",
    "class BlockRegional(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, L, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        D = L // n_head\n",
    "        self.sa = RegionalMultiHeadAttention(n_head, D, C, L)\n",
    "        self.ffwd = FeedFowardRegional(L)\n",
    "        self.ln1 = nn.LayerNorm(L).double()\n",
    "        self.ln2 = nn.LayerNorm(L).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "# ============================================\n",
    "# SHAPE CHANGER FOR COMPATIBILITY WITH SYNCHRONOUS ENCODER\n",
    "# ============================================\n",
    "def RegionalToSynchronousShapeShifter(tensor):\n",
    "    b, s, c, l = tensor.shape\n",
    "    return tensor.view(b, c, s, l)\n",
    "\n",
    "# ============================================\n",
    "# SYNCHRONOUS ENCODER COMPONENTS\n",
    "# ============================================\n",
    "class SynchronousHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, S, L):\n",
    "        # Parameters are head_size(d), no. of tokens(C), and input embedding size(L)\n",
    "        super().__init__()\n",
    "        self.block_size = S\n",
    "        self.n_embed = L\n",
    "        self.head_size = head_size \n",
    "        self.key = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
    "        self.query = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
    "        self.value = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(self.block_size, self.block_size)))\n",
    "        self.dropout = nn.Dropout(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, s, l= x.shape\n",
    "        x = x.view(c, b, s, l)  # (C, B, S, L)\n",
    "        matrices = []\n",
    "\n",
    "        for spatial_mat in x:\n",
    "            inp = spatial_mat\n",
    "            k = self.key(inp)   # (B, S, D)\n",
    "            q = self.query(inp) # (B, S, D)\n",
    "            # compute attention scores (\"affinities\")\n",
    "            wei = q @ k.transpose(-2,-1) * self.head_size**-0.5 # (B, S, D) @ (B, D, S) -> (B, S, S)\n",
    "            wei = F.softmax(wei, dim=-1) # (B, S, S)\n",
    "            wei = self.dropout(wei)\n",
    "            # perform the weighted aggregation of the values\n",
    "            v = self.value(inp) # (B, S, D)\n",
    "            out = wei @ v # (B, S, S) @ (B, S, D) -> (B, S, D)\n",
    "            matrices.append(out.tolist())\n",
    "\n",
    "        matrices = torch.tensor(matrices)\n",
    "        out = matrices.view(b, c, s, self.head_size)\n",
    "        return out.double().to(device)\n",
    "    \n",
    "class SynchronousMultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, S, L):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SynchronousHead(head_size, S, L) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(L, L).double()\n",
    "        self.dropout = nn.Dropout(0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1).to('cuda')\n",
    "        # out = self.dropout(self.proj(out)) # Instead of this line, we proceed as below:\n",
    "\n",
    "        # Implementing projection layer after the multihead attention module\n",
    "        b, c, s, l = out.shape\n",
    "        out = out.view(c, b, s, l)\n",
    "\n",
    "        matrices = []\n",
    "        for inp in out:\n",
    "            matrix = self.dropout(self.proj(inp))  # inp is (B, S, L)\n",
    "            matrices.append(matrix.tolist())\n",
    "\n",
    "        matrices = torch.tensor(matrices)\n",
    "        matrices = matrices.view(b, c, s, l)\n",
    "\n",
    "        return matrices.double().to(device)\n",
    "    \n",
    "class FeedFowardSync(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, L): \n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(L, 4*L), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*L, L),\n",
    "            nn.Dropout(dropout),\n",
    "        ).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, s, l = x.shape\n",
    "        x = x.view(c, b, s, l)\n",
    "\n",
    "        matrices = []\n",
    "        for inp in x:\n",
    "            matrix = self.net(inp)\n",
    "            matrices.append(matrix.tolist())\n",
    "\n",
    "        matrices = torch.tensor(matrices)\n",
    "        # s_, b_, c_, l_ = matrices.shape\n",
    "        matrices = matrices.view(b, c, s, l)\n",
    "\n",
    "\n",
    "        return matrices.double().to(device)\n",
    "    \n",
    "class BlockSync(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, L, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        D = L // n_head\n",
    "        self.sa = SynchronousMultiHeadAttention(n_head, D, S, L)\n",
    "        self.ffwd = FeedFowardSync(L)\n",
    "        self.ln1 = nn.LayerNorm(L).double()\n",
    "        self.ln2 = nn.LayerNorm(L).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "# ============================================\n",
    "# TEMPORAL ENCODER COMPONENTS\n",
    "# ============================================\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, S, C, L, M):\n",
    "        super(TemporalTransformer, self).__init__()\n",
    "        self.C = C  # Number of channels\n",
    "        self.L = L  # Original temporal dimensionality\n",
    "        self.S = S  # Spatial dimension\n",
    "        self.M = M  # Compressed dimensionality\n",
    "        \n",
    "        self.patch_size = self.C * self.S  # Patch size\n",
    "        self.M_linear = nn.Linear(self.patch_size, self.patch_size).double()  # Learnable matrix M\n",
    "        \n",
    "    def forward(self, z5):\n",
    "        # z5: (B, C, S, D) input tensor\n",
    "        # Recuce the temporal dimension to M\n",
    "        z5_averaged = self.reduce_temporal_dimension(z5, self.M) # (B, C, S, M)\n",
    "        # Reshape the tensor to B, M, S*C\n",
    "        z5_reshaped = z5_averaged.reshape(z5.shape[0], -1, self.S*self.C)  # B, M, S*C\n",
    "        # Get latent vectors out of the current tensor\n",
    "        latent = self.M_linear(z5_reshaped) # (B, M, S*C)\n",
    "        return latent\n",
    "    \n",
    "    def reduce_temporal_dimension(self, input_tensor, M):\n",
    "        # input_tensor: (B, C, S, L) input tensor\n",
    "        # M: Compressed dimensionality\n",
    "\n",
    "        # Reshape the tensor to 3D\n",
    "        reshaped_tensor = input_tensor.view(-1, input_tensor.size(2), input_tensor.size(3))  # Shape: (B*C, S, L)\n",
    "\n",
    "        # Calculate the mean along the last dimension (L)\n",
    "        averaged_tensor = torch.mean(reshaped_tensor, dim=-1)  # Shape: (B*C, S)\n",
    "\n",
    "        # Resize the tensor to have the desired compressed dimensionality (M)\n",
    "        resized_tensor = torch.nn.functional.interpolate(averaged_tensor.unsqueeze(-1), size=M, mode='linear', align_corners=False)\n",
    "        resized_tensor = resized_tensor.squeeze(-1)\n",
    "\n",
    "        # Reshape back to 4D\n",
    "        output_tensor = resized_tensor.view(input_tensor.size(0), input_tensor.size(1), input_tensor.size(2), M)  # Shape: (B, C, S, M)\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "class HeadTemporal(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embed):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False).double()\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False).double()\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False).double()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttentionTemporal(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embed):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.heads = nn.ModuleList([HeadTemporal(head_size, self.n_embed) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(self.n_embed, self.n_embed).double()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFowardTemporal(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        ).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttentionTemporal(n_head, head_size, n_embd)\n",
    "        self.ffwd = FeedFowardTemporal(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd).double()\n",
    "        self.ln2 = nn.LayerNorm(n_embd).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "# UTILITY FUNCTION NEEDED FOR TEMPORAL ENCODER\n",
    "def product_of_2_least_common_factors(num):\n",
    "    factors = []\n",
    "    \n",
    "    # Find all factors of the number\n",
    "    for i in range(1, num + 1):\n",
    "        if num % i == 0:\n",
    "            factors.append(i)\n",
    "        if len(factors) == 3:\n",
    "            break\n",
    "    \n",
    "    ans = 1\n",
    "    for factor in factors:\n",
    "        ans = ans * factor\n",
    "    \n",
    "    return ans\n",
    "\n",
    "# ============================================\n",
    "# DECODER\n",
    "# ============================================\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, B, M, S, C):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.B = B\n",
    "        self.M = M\n",
    "        self.S = S\n",
    "        self.C = C\n",
    "\n",
    "        # Define the layers\n",
    "        # Define the 1D convolutional filter - captures info along the convolutional dimension\n",
    "        self.l1_filter = nn.Conv1d(M*S, M*S, kernel_size=C).double()\n",
    "        # Define the l2 filter - captures info along spatial dimension\n",
    "        self.l2_filter = nn.Conv1d(M, M, kernel_size=S).double()\n",
    "        # PREDICTION NEURAL NETWORK\n",
    "        self.layer0 = nn.Linear(M, 256).double()\n",
    "        self.layer1 = nn.Linear(256, 64).double()\n",
    "        self.layer2 = nn.Linear(64, 1).double()\n",
    "        self.leaky_relu = nn.LeakyReLU().double()\n",
    "        self.sigmoid = nn.Sigmoid().double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder_to_decoder_shape_transition(x)\n",
    "\n",
    "        # Reshape from (B, M, S, C) to (B, M*S, C)\n",
    "        x = x.view(self.B, self.M*self.S, self.C)\n",
    "        # Apply the convolutional filter\n",
    "        x = self.l1_filter(x)  # reduces C dimension to 1\n",
    "        # Reshape the output tensor back to the desired shape (B, M, S)\n",
    "        x = x.view(self.B, self.M, self.S)\n",
    "        # apply\n",
    "        x = self.l2_filter(x)  # this filter reduces s dimension to 1\n",
    "        # Reshape\n",
    "        b, m, s =  x.shape \n",
    "        x = x.view(b, m*s)  # (B, M)\n",
    "    \n",
    "        # Pass the input through the layers with activations\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def encoder_to_decoder_shape_transition(self, matrix):\n",
    "        '''this function reshapes the oupput of encoder so that it is\n",
    "        suitable for the decoder'''\n",
    "        matrix = matrix.view(B, M, S, C)\n",
    "        return matrix\n",
    "    \n",
    "# ============================================\n",
    "# EEGFORMER\n",
    "# ============================================\n",
    "class EEGFormer(nn.Module):\n",
    "    def __init__(self, B, S, C, L, M):\n",
    "        super().__init__()\n",
    "        self.B = B\n",
    "        self.S = S\n",
    "        self.C = C\n",
    "        self.L = L\n",
    "        self.M = M\n",
    "        self.position_embedding_table = nn.Embedding(L, 1)\n",
    "        self.conv1d_layer = CNN1D(S=S, L=L, C=C)\n",
    "        self.br = BlockRegional(L, num_heads)\n",
    "        self.bs = BlockSync(L, num_heads)\n",
    "        self.temporal = TemporalTransformer(S, C, L, M=M) \n",
    "        self.bt = TemporalBlock(S*C, n_head=product_of_2_least_common_factors(S*C))  # nembd, nhead\n",
    "        self.decoder = Decoder(B, M, S, C)\n",
    "        self.position_embedding_table = nn.Embedding(L, 1)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x is eeg segment\n",
    "        # x = x + self.position_embedding_table(x.long()).squeeze().double()\n",
    "        x = self.conv1d_layer(x)\n",
    "        x = pad_tensor(x, dim=3, length=num_cols//128)\n",
    "        x = self.br(x)\n",
    "        x = RegionalToSynchronousShapeShifter(x)\n",
    "        x = self.bs(x)\n",
    "        x = self.temporal(x)\n",
    "        x = self.bt(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, cols = x.shape\n",
    "            probabilities = x.view(B*cols,)\n",
    "            loss = F.binary_cross_entropy(probabilities, targets)   \n",
    "                     \n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate F1 score, precision, and recall for both classes\n",
    "def calculate_metrics(predictions, targets, num_classes=2):\n",
    "    predictions = torch.tensor(predictions).int()\n",
    "    targets = torch.tensor(targets).int()\n",
    "\n",
    "    confusion_matrix = [[0 for _ in range(num_classes)] for _ in range(num_classes)]\n",
    "\n",
    "    for p, t in zip(predictions, targets):\n",
    "        confusion_matrix[t][p] += 1\n",
    "\n",
    "    metrics_per_class = {}\n",
    "    for i in range(num_classes):\n",
    "        true_positives = confusion_matrix[i][i]\n",
    "        false_positives = sum(confusion_matrix[j][i] for j in range(num_classes) if j != i)\n",
    "        false_negatives = sum(confusion_matrix[i][j] for j in range(num_classes) if j != i)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        metrics_per_class[f'c{i}_Precision'] = precision\n",
    "        metrics_per_class[f'c{i}_Recall'] = recall\n",
    "        metrics_per_class[f'c{i}_F1_Score'] = f1_score\n",
    "\n",
    "    return metrics_per_class\n",
    "\n",
    "# Updated Loss Estimator\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'eval']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(t, test=True) \n",
    "            probs, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "            # Convert probabilities to class predictions\n",
    "            predictions = (probs > 0.5).int()\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(Y.cpu().numpy())\n",
    "\n",
    "        out[split] = {\n",
    "            'loss': losses.mean(),\n",
    "            **calculate_metrics(all_predictions, all_targets)  # Include metrics in the output\n",
    "        }\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(data_folders,\n",
    "            n_recordings,\n",
    "            sensor_types,\n",
    "            n_chans,\n",
    "            max_recording_mins,\n",
    "            sec_to_cut, duration_recording_mins,\n",
    "            test_recording_mins,\n",
    "            max_abs_val,\n",
    "            sampling_freq,\n",
    "            divisor,\n",
    "            test_on_eval,\n",
    "            n_folds, i_test_fold,\n",
    "            shuffle,\n",
    "            model_name,\n",
    "            n_start_chans, n_chan_factor,\n",
    "            input_time_length, final_conv_length,\n",
    "            model_constraint,\n",
    "            init_lr,\n",
    "            batch_size, max_epochs,cuda, num_cols):\n",
    "    # WRITE PREPROCESSING FUNCTIONS\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    preproc_functions = []\n",
    "    # cut seconds from start and end\n",
    "    preproc_functions.append(\n",
    "        lambda data, fs: (data[:, int(sec_to_cut * fs):-int(\n",
    "            sec_to_cut * fs)], fs))\n",
    "    # crop recording to maximum duration min\n",
    "    preproc_functions.append(\n",
    "        lambda data, fs: (data[:, :int(duration_recording_mins * 60 * fs)], fs))\n",
    "    # Clipping the data\n",
    "    if max_abs_val is not None:\n",
    "        preproc_functions.append(lambda data, fs:\n",
    "                                 (np.clip(data, -max_abs_val, max_abs_val), fs))\n",
    "    #my edit due to ValueError: Input signal length=0 is too small to resample from 250.0->100     if data.shape[1]!=0:\n",
    "    preproc_functions.append(lambda data, fs: (resampy.resample(data, fs,\n",
    "                                                                sampling_freq,\n",
    "                                                                axis=1,\n",
    "                                                                filter='kaiser_fast'),\n",
    "                                               sampling_freq))\n",
    "    if divisor is not None:\n",
    "        preproc_functions.append(lambda data, fs: (data / divisor, fs))\n",
    "    preproc_functions.append(lambda arr, sampling_freq: \\\n",
    "            (np.pad(arr, ((0, n_chans - arr.shape[0]), (0, num_cols - arr.shape[1])),\n",
    "            mode='constant', constant_values=-100), sampling_freq))\n",
    "\n",
    "    # MAKE DATASET OBJECTS\n",
    "    dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                           max_recording_mins=max_recording_mins,\n",
    "                           preproc_functions=preproc_functions,\n",
    "                           data_folders=data_folders,\n",
    "                           train_or_eval='train',\n",
    "                           sensor_types=sensor_types)\n",
    "    if test_on_eval:\n",
    "        if test_recording_mins is None:\n",
    "            test_recording_mins = duration_recording_mins\n",
    "        test_preproc_functions = copy(preproc_functions)\n",
    "        test_preproc_functions[1] = lambda data, fs: (\n",
    "            data[:, :int(test_recording_mins * 60 * fs)], fs)\n",
    "        test_dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                                max_recording_mins=None,\n",
    "                                preproc_functions=test_preproc_functions,\n",
    "                                data_folders=data_folders,\n",
    "                                train_or_eval='eval',\n",
    "                                sensor_types=sensor_types)\n",
    "        \n",
    "    # LOAD DATA FROM DATASET OBJECTS\n",
    "    X,y = dataset.load()\n",
    "    max_shape = np.max([list(x.shape) for x in X],\n",
    "                       axis=0)\n",
    "    # assert max_shape[1] == int(duration_recording_mins *\n",
    "                            #    sampling_freq * 60)\n",
    "    if test_on_eval:\n",
    "        test_X, test_y = test_dataset.load()\n",
    "        max_shape = np.max([list(x.shape) for x in test_X],\n",
    "                           axis=0)\n",
    "        #assert max_shape[1] == int(test_recording_mins *   sampling_freq * 60)\n",
    "    if not test_on_eval:\n",
    "        splitter = TrainValidTestSplitter(n_folds, i_test_fold,\n",
    "                                          shuffle=shuffle)\n",
    "        train_set, valid_set, test_set = splitter.split(X, y)\n",
    "    else:\n",
    "        splitter = TrainValidSplitter(n_folds, i_valid_fold=i_test_fold,\n",
    "                                          shuffle=shuffle)\n",
    "        train_set, valid_set = splitter.split(X, y)\n",
    "        test_set = SignalAndTarget(test_X, test_y)\n",
    "        del test_X, test_y\n",
    "    del X,y # shouldn't be necessary, but just to make sure\n",
    "\n",
    "    # AT THIS POINT WE HAVE TRAIN SET, TEST SET, VALIDATION SET\n",
    "\n",
    "    set_random_seeds(seed=20170629, cuda=cuda)\n",
    "    n_classes = 2\n",
    "    if model_name == 'linear':\n",
    "        model = nn.Sequential()\n",
    "        model.add_module(\"conv_classifier\",\n",
    "                         nn.Conv2d(n_chans, n_classes, (600,1)))\n",
    "        model.add_module('softmax', nn.LogSoftmax())\n",
    "        model.add_module('squeeze', Expression(lambda x: x.squeeze(3)))\n",
    "    elif model_name == 'transformer':\n",
    "        model = EEGFormer(B, S, C, L, M)\n",
    "    else:\n",
    "        assert False, \"unknown model name {:s}\".format(model_name)\n",
    "\n",
    "    return train_set, valid_set, test_set, model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 127400)\n",
      "(21, 103400)\n",
      "(21, 103400)\n",
      "(21, 103400)\n",
      "(21, 51700)\n",
      "(21, 51700)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 144000)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 168000)\n",
      "(21, 144000)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 137200)\n",
      "(21, 113200)\n",
      "(21, 113200)\n",
      "(21, 113200)\n",
      "(21, 56600)\n",
      "(21, 56600)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 184400)\n",
      "(21, 160400)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 168000)\n",
      "(21, 144000)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 164400)\n",
      "(21, 140400)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 182400)\n",
      "(21, 158400)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 129800)\n",
      "(21, 105800)\n",
      "(21, 105800)\n",
      "(21, 105800)\n",
      "(21, 52900)\n",
      "(21, 52900)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 156400)\n",
      "(21, 132400)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 120400)\n",
      "(21, 96400)\n",
      "(21, 96400)\n",
      "(21, 96400)\n",
      "(21, 48200)\n",
      "(21, 48200)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 151000)\n",
      "(21, 127000)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 125200)\n",
      "(21, 101200)\n",
      "(21, 101200)\n",
      "(21, 101200)\n",
      "(21, 50600)\n",
      "(21, 50600)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 191000)\n",
      "(21, 167000)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 122200)\n",
      "(21, 98200)\n",
      "(21, 98200)\n",
      "(21, 98200)\n",
      "(21, 49100)\n",
      "(21, 49100)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 151800)\n",
      "(21, 127800)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 192000)\n",
      "(21, 168000)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 120200)\n",
      "(21, 96200)\n",
      "(21, 96200)\n",
      "(21, 96200)\n",
      "(21, 48100)\n",
      "(21, 48100)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 132000)\n",
      "(21, 108000)\n",
      "(21, 108000)\n",
      "(21, 108000)\n",
      "(21, 54000)\n",
      "(21, 54000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 156600)\n",
      "(21, 132600)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 180800)\n",
      "(21, 156800)\n",
      "(21, 120000)\n",
      "(21, 120000)\n",
      "(21, 60000)\n",
      "(21, 60000)\n",
      "['A1']\n",
      "['A2']\n",
      "['C3']\n",
      "['C4']\n",
      "['CZ']\n",
      "['F3']\n",
      "['F4']\n",
      "['F7']\n",
      "['F8']\n",
      "['FP1']\n",
      "['FP2']\n",
      "['FZ']\n",
      "['O1']\n",
      "['O2']\n",
      "['P3']\n",
      "['P4']\n",
      "['PZ']\n",
      "['T3']\n",
      "['T4']\n",
      "['T5']\n",
      "['T6']\n",
      "(21, 139200)\n",
      "(21, 115200)\n",
      "(21, 115200)\n",
      "(21, 115200)\n",
      "(21, 57600)\n",
      "(21, 57600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82115/997940939.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  return (torch.tensor(new_X).double().to(device), torch.tensor(new_y).double().to(device))\n"
     ]
    }
   ],
   "source": [
    "t, v, te, model = run_exp(data_folders,\n",
    "            n_recordings,\n",
    "            sensor_types,\n",
    "            n_chans,\n",
    "            max_recording_mins,\n",
    "            sec_to_cut, duration_recording_mins,\n",
    "            test_recording_mins,\n",
    "            max_abs_val,\n",
    "            sampling_freq,\n",
    "            divisor,\n",
    "            test_on_eval,\n",
    "            n_folds, i_test_fold,\n",
    "            shuffle,\n",
    "            model_name,\n",
    "            n_start_chans, n_chan_factor,\n",
    "            input_time_length, final_conv_length,\n",
    "            model_constraint,\n",
    "            init_lr,\n",
    "            batch_size, max_epochs,cuda, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_list(original_list, segment_length):\n",
    "    segmented_list = []\n",
    "    for element in original_list:\n",
    "        # Calculate the number of segments that can be obtained from the element\n",
    "        num_segments = element.size(1) // segment_length\n",
    "        # Split the element into segments of specified length along the second axis\n",
    "        segments = torch.split(element[:, :num_segments * segment_length], segment_length, dim=1)\n",
    "        # Append the segments to the segmented list\n",
    "        segmented_list.extend(segments)\n",
    "    return torch.stack(segmented_list)\n",
    "\n",
    "def get_batch(dataset, test=False):\n",
    "    global start\n",
    "    if not test:\n",
    "        # data set is a tuple: (list of x, list of y)\n",
    "        x = segment_list([dataset[0][start]], int(60032/128))\n",
    "        y = dataset[1][start].repeat(1, 128).view(128,)\n",
    "        \n",
    "        start += 1\n",
    "    if test:\n",
    "        # data set is a tuple: (list of x, list of y)\n",
    "        max_i = len(dataset[0])\n",
    "        ind = torch.randint(0, max_i, (1,)).item()\n",
    "        x = segment_list([dataset[0][ind]], int(60032/128))\n",
    "        y = dataset[1][ind].repeat(1, 128).view(128,)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the training....................\n",
      "iter:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Every once in a while, evaluate the loss, F1 score, precision, and recall on train and val sets\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39m%\u001b[39m eval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39m==\u001b[39m max_iters \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 17\u001b[0m     losses_and_metrics \u001b[39m=\u001b[39m estimate_loss()\n\u001b[1;32m     18\u001b[0m     train_loss \u001b[39m=\u001b[39m losses_and_metrics[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m     val_loss \u001b[39m=\u001b[39m losses_and_metrics[\u001b[39m'\u001b[39m\u001b[39meval\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[46], line 39\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(eval_iters):\n\u001b[1;32m     38\u001b[0m     X, Y \u001b[39m=\u001b[39m get_batch(t, test\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \n\u001b[0;32m---> 39\u001b[0m     probs, loss \u001b[39m=\u001b[39m model(X, Y)\n\u001b[1;32m     40\u001b[0m     losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     42\u001b[0m     \u001b[39m# Convert probabilities to class predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[45], line 473\u001b[0m, in \u001b[0;36mEEGFormer.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m    471\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1d_layer(x)\n\u001b[1;32m    472\u001b[0m x \u001b[39m=\u001b[39m pad_tensor(x, dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, length\u001b[39m=\u001b[39mnum_cols\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m--> 473\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbr(x)\n\u001b[1;32m    474\u001b[0m x \u001b[39m=\u001b[39m RegionalToSynchronousShapeShifter(x)\n\u001b[1;32m    475\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbs(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[45], line 142\u001b[0m, in \u001b[0;36mBlockRegional.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    141\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln1(x))\n\u001b[0;32m--> 142\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mffwd(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln2(x))\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[45], line 119\u001b[0m, in \u001b[0;36mFeedFowardRegional.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m x:\n\u001b[1;32m    118\u001b[0m     matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(inp)\n\u001b[0;32m--> 119\u001b[0m     matrices\u001b[39m.\u001b[39mappend(matrix\u001b[39m.\u001b[39;49mtolist())\n\u001b[1;32m    121\u001b[0m matrices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(matrices)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    122\u001b[0m \u001b[39m# s_, b_, c_, l_ = matrices.shape\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#==================================\n",
    "# TRAINING LOOP\n",
    "#==================================\n",
    "# Create a list to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "start = 0\n",
    "# Training loop\n",
    "print('starting the training....................')\n",
    "for iter in range(len(t[0])):  # len(t[0]) reresents no. of elements in training set\n",
    "    print('iter: ', iter)\n",
    "    # Every once in a while, evaluate the loss, F1 score, precision, and recall on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses_and_metrics = estimate_loss()\n",
    "        train_loss = losses_and_metrics['train']['loss']\n",
    "        val_loss = losses_and_metrics['eval']['loss']\n",
    "        print(f\"Step {iter}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}\")\n",
    "\n",
    "        # Append losses to the lists\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Log the losses and metrics to a text file\n",
    "        with open(log_file_path, \"a\") as log_file:\n",
    "            log_file.write(f\"Step {iter}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}\\n\")\n",
    "            for metric_name, metric_value in losses_and_metrics['eval'].items():\n",
    "                log_file.write(f\"{metric_name.capitalize()}: {metric_value:.4f}\\n\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch(t)\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "# TRAINING LOOP with additional loop \n",
    "#==================================\n",
    "# Create a list to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "start = 0\n",
    "\n",
    "# Training loop\n",
    "for _ in range(max_epochs):\n",
    "    start = 0\n",
    "    for iter in range(len(t[0])):  # len(t[0]) reresents no. of elements in training set\n",
    "        # Every once in a while, evaluate the loss, F1 score, precision, and recall on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses_and_metrics = estimate_loss()\n",
    "            train_loss = losses_and_metrics['train']['loss']\n",
    "            val_loss = losses_and_metrics['eval']['loss']\n",
    "            print(f\"Step {iter}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}\")\n",
    "\n",
    "            # Append losses to the lists\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # Log the losses and metrics to a text file\n",
    "            with open(log_file_path, \"a\") as log_file:\n",
    "                log_file.write(f\"Step {iter}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}\\n\")\n",
    "                for metric_name, metric_value in losses_and_metrics['eval'].items():\n",
    "                    log_file.write(f\"{metric_name.capitalize()}: {metric_value:.4f}\\n\")\n",
    "\n",
    "        # Sample a batch of data\n",
    "        xb, yb = get_batch(t)\n",
    "\n",
    "        # Evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
